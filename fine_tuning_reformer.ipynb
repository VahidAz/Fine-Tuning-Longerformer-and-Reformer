{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a123078-9244-4656-8df4-cb15fe3acac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This is a sample code snippet for fine-tuning a Reformer model.\n",
    "### https://huggingface.co/docs/transformers/en/model_doc/reformer\n",
    "### https://huggingface.co/google/reformer-crime-and-punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb1b8e-383e-4561-9414-d60dc5cf6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fine_tune_reformer.py\n",
    "\n",
    "\n",
    "# Lib versions\n",
    "# transformers_version='4.6'\n",
    "# pytorch_version='1.6'\n",
    "# py_version='py36'\n",
    "\n",
    "\n",
    "# Headers\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import pickle as plk\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import (\n",
    "    ReformerForMaskedLM,\n",
    "    ReformerTokenizer,\n",
    "    ReformerConfig,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "# Load the data\n",
    "# Provide the path to your data. Here, we assume the data is a list, as shown in the example below\n",
    "# [{'text': 'Large Transformer models routinely achieve state-of-the-art results on a number of tasks ...',\n",
    "#   'id': 0}, \n",
    "#  {'text': 'The resulting model, the Reformer, performs on par with Transformer models while being ...',\n",
    "#   'id': 1},]\n",
    "data_location = \"YOUR_DATA_PATH\"\n",
    "with open(data_location, \"rb\") as fin:\n",
    "    all_texts = plk.load(fin)\n",
    "df_train = pd.DataFrame(all_texts)\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "MODEL_CKPT = \"google/reformer-crime-and-punishment\"\n",
    "tokenizer = ReformerTokenizer.from_pretrained(MODEL_CKPT)\n",
    "tokenizer.add_special_tokens({\"mask_token\": '[MASK]'})\n",
    "# print(tokenizer.mask_token_id)\n",
    "# len(tokenizer)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# print(tokenizer.pad_token_id)\n",
    "# len(tokenizer)\n",
    "\n",
    "\n",
    "# Set the sequence length\n",
    "sequence_length = 2 ** 14\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "def tokenize_function(batched_data):\n",
    "    result = tokenizer(batched_data['text'], pad_to_max_length=True,\n",
    "                       max_length=sequence_length, return_attention_mask=True,\n",
    "                       padding='max_length', truncation=True, return_token_type_ids=False)\n",
    "    # if tokenizer.is_fast:\n",
    "    #     result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'id'])\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "dataset_split = tokenized_datasets.train_test_split(test_size=0.1) # Split the dataset to train and test\n",
    "\n",
    "\n",
    "# Load the Reformer model with the below architecture\n",
    "config = {\n",
    "  \"attention_head_size\": 128,\n",
    "  \"attn_layers\": [\n",
    "    \"local\",\n",
    "    \"local\",\n",
    "    \"lsh\",\n",
    "    \"local\",\n",
    "    \"local\",\n",
    "    \"local\",\n",
    "    \"lsh\",\n",
    "    \"local\",\n",
    "    \"local\",\n",
    "    \"local\",\n",
    "    \"lsh\",\n",
    "    \"local\"\n",
    "  ],\n",
    "  \"axial_norm_std\": 1.0,\n",
    "  \"axial_pos_embds\": True,\n",
    "  \"axial_pos_embds_dim\": [\n",
    "    256,\n",
    "    768\n",
    "  ],\n",
    "  \"axial_pos_shape\": [\n",
    "    128,\n",
    "    128\n",
    "  ],\n",
    "  \"chunk_size_feed_forward\": 0,\n",
    "  \"chunk_size_lm_head\": 0,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"feed_forward_size\": 4096,\n",
    "  \"hidden_act\": \"relu\",\n",
    "  \"hidden_dropout_prob\": 0.2,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"is_decoder\": False,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"local_attention_probs_dropout_prob\": 0.2,\n",
    "  \"local_attn_chunk_length\": 128,\n",
    "  \"local_num_chunks_after\": 0,\n",
    "  \"local_num_chunks_before\": 1,\n",
    "  \"lsh_attention_probs_dropout_prob\": 0.1,\n",
    "  \"lsh_attn_chunk_length\": 256,\n",
    "  \"lsh_num_chunks_after\": 0,\n",
    "  \"lsh_num_chunks_before\": 1,\n",
    "  \"max_position_embeddings\": 16384,\n",
    "  \"model_type\": \"reformer\",\n",
    "  \"num_attention_heads\": 8,\n",
    "  \"num_buckets\": 512,\n",
    "  \"num_hashes\": 1,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"vocab_size\": 323  # +1 for [MASK] token\n",
    "}\n",
    "config = ReformerConfig(**config)\n",
    "model = ReformerForMaskedLM(config)\n",
    "model = model.train()\n",
    "\n",
    "\n",
    "# Define the training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"YOUR_OUTPUT_PATH\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,    \n",
    "    per_device_eval_batch_size= 2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.001,\n",
    "    logging_steps=4,\n",
    "    save_steps=20,\n",
    "    fp16=True,\n",
    "    # logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_split[\"train\"],\n",
    "    eval_dataset=dataset_split[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate(eval_dataset=dataset_split[\"test\"])\n",
    "\n",
    "\n",
    "# Saves the model\n",
    "save_path = \"YOUT_SAVE_PATH\"\n",
    "trainer.save_model(save_path)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
